# transcription-summarization

# ğŸ§  Speech-to-Text + Summarization  
**UCU NLP Course Project â€” â€œNLP Fairiesâ€ (Lida, Khrystya, Yulia)**  
ğŸ“… *November 2025*

---

## ğŸ“– Overview

This project implements an end-to-end system that:
1. Converts **spoken audio** into text using **OpenAIâ€™s Whisper** model.  
2. Summarizes the resulting transcripts using an **abstractive transformer-based summarizer (BART-Large-CNN)**.  

ğŸ¯ **Goal:** Automate the process of transcribing and condensing multi-speaker conversations (e.g., meetings or interviews) into short, coherent summaries.

---

## ğŸ’¡ Motivation

Every day, hours of speech are recorded in meetings, podcasts, and lectures â€” but most of it remains **unstructured**.  
Manual transcription and summarization are **slow**, **error-prone**, and **expensive**.  

An integrated **STT + Summarization** system makes spoken content:
- ğŸ§­ Searchable  
- ğŸ’¬ Easy to digest  
- ğŸ“Š Ready for analysis or downstream NLP tasks  

---

## ğŸ—‚ï¸ Data Overview

### ğŸ§ Speech-to-Text dataset
- **Source:** FLEURS (clean subset)  
- **Languages:** English / Ukrainian  
- **Size:** 1,817 audioâ€“text pairs (~4â€“5 hours total)  
- **Splits:** 80 % train | 10 % validation | 10 % test  
- **Average clip duration:** 6â€“12 seconds  
- **Average transcript length:** 10â€“20 words  

### ğŸ“° Summarization dataset
- **Source:** [SAMSum dataset](https://huggingface.co/datasets/knkarthick/samsum)  
- **Domain:** multi-speaker chat dialogues  
- **Purpose:** fine-tuning the BART-Large-CNN summarizer  

---

## ğŸ§¹ Data Cleaning & Preprocessing

### Audio
- Converted all files â†’ WAV (16 kHz mono)  
- Trimmed silence / normalized volume  
- Removed clips < 1 s or > 30 s  

### Text
- Lowercased + removed fillers and punctuation  
- Ensured alignment between audio and transcript  
- Split by speaker to avoid leakage between splits  

---

## ğŸ§© Model Architecture

### 1ï¸âƒ£ Speech-to-Text: Whisper
- Pretrained multilingual encoderâ€“decoder model from **OpenAI**  
- **Input:** log-mel spectrograms  
- **Output:** tokenized text sequences  
- Fine-tuned with frozen encoder to prevent overfitting on small data  
- Variable-length padding + masking for stable training  

### 2ï¸âƒ£ Summarization: BART-Large-CNN
- Transformer encoderâ€“decoder for **abstractive summarization**  
- Fine-tuned on **SAMSum** dialogues  
- Outputs short, human-like summaries of multi-speaker text  

---

## ğŸ§® Training Pipeline
COLLECT & SPLIT DATA
â†“
PREPROCESS AUDIO + TEXT
â†“
LOAD MODEL CONFIGURATIONS
â†“
FREEZE ENCODER (Whisper)
â†“
BATCH, PAD, MASK
â†“
TRAIN + VALIDATE



---

## âš™ï¸ Model Inference Example

**Expected:**  
> â€œUN peacekeepers who arrived in Haiti after the 2010 earthquake are being blamed for the spread of the disease which started near the troopsâ€™ encampment.â€

**Predicted:**  
> â€œYou and peacekeepers who arrived in Hady after the 2010 earthquake are being blamed for the spread of the disease which started near the troops encampment.â€

ğŸ—£ï¸ *Close semantic match but minor pronunciation errors (Haiti â†’ Hady).*

---

## ğŸ“° Summarization Example

**Input dialogue:**  
> A: â€œHey, did you finish the meeting notes?â€  
> B: â€œNot yet, Iâ€™ll summarize them later.â€  

**Generated summary:**  
> â€œThey discussed finishing the meeting notes later.â€  

---

## âš ï¸ Challenges

| Type | Description | Mitigation |
|------|--------------|-------------|
| ğŸ§ Audio quality | Background noise, variable loudness | Normalization + silence trimming |
| ğŸ§¾ Text mismatch | Misalignment between audio & transcripts | Regex cleaning + manual spot-check |
| âš–ï¸ Imbalance | Variable clip lengths (1â€“30 s) | Quartile grouping + batching |
| ğŸ’» Runtime limits | GPU memory & Colab timeouts | Checkpointing + smaller batch sizes |
| ğŸ§© Multilingual noise | Mixed EN/UA samples | Language-specific filtering |
| âœ Summarization quality | Context loss in dialogues | Fine-tuning + ROUGE evaluation |

---

## ğŸ“ˆ Results (Preliminary)

| Model | Metric | Score |
|-------|---------|--------|
| Whisper (STT) | Word Error Rate (WER) | ~0.18â€“0.22 |
| BART-Large-CNN (Summary) | ROUGE-1 / ROUGE-L | ~0.42 / 0.39 |

*(Approximate scores based on validation subset.)*

---

## ğŸš€ Next Steps

1ï¸âƒ£ **Integrate everything into one pipeline**  
   â†’ ğŸ™ï¸ Audio â†’ ğŸ§  STT â†’ ğŸ§¹ Preprocessing â†’ ğŸ“° Summarization â†’ âœ… Output  

2ï¸âƒ£ **Optimize model and data**  
   â†’ Add more data via **augmentation** (noise, speed, pitch)  
   â†’ Clean noisy/long clips ğŸ§½  
   â†’ Try smaller & faster models âš¡  

3ï¸âƒ£ **Evaluate the final solution**  
   â†’ Compute **WER / CER** for STT  
   â†’ Compute **ROUGE / BERTScore** for summarization  
   â†’ Compare baseline vs improved results ğŸ“Š   

---

## ğŸ‘©â€ğŸ’» Contributors

| Name | Role |
|------|------|
| **Lida** | Data preprocessing & EDA |
| **Khrystya** | Speech-to-Text (Whisper) |
| **Yulia** | Summarization (BART-Large-CNN) |

---

## ğŸ§¾ References
- OpenAI Whisper (2022) â€” [GitHub](https://github.com/openai/whisper)  
- Hugging Face Transformers (BART-Large-CNN)  
- SAMSum Dataset (2019) â€” Dialogue Summarization Benchmark  
- FLEURS Dataset (Google Research, 2022)  
